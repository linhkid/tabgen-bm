import random
from collections import deque
from itertools import permutations

import networkx as nx
import torch as T
import numpy as np
from .utils.buffer import ReplayBuffer
from warnings import simplefilter

from pgmpy.models import BayesianNetwork

from pgmpy.metrics import log_likelihood_score
from pgmpy.estimators import (
    AIC,
    BDeu,
    BDs,
    BIC,
    K2,
    ScoreCache,
    StructureEstimator,
    StructureScore,
)


simplefilter(action="ignore", category=FutureWarning)

device = T.device("cuda:0" if T.cuda.is_available() else "cpu")

if T.backends.mps.is_available():
    device = T.device("mps")


class ReinforcementLearningAgent:
    def __init__(self, alpha=0.1, gamma=0.9, epsilon=0.3, max_size=1000000, batch_size=256, data_X=None, data_Y= None, state_dim=0,
                 action_dim=0, fc1_dim=0, fc2_dim=0, fc3_dim=0, ckpt_dir='./checkpoints/DDQN',
                 tau=0.005, ):
        #gamma 0.8 0.9

        # Q(S,A) + a*(r*maxQ(S',A')-Q(S,A))
        self.alpha = alpha  # The Learning Rate
        self.gamma = gamma  # The Discount Factor
        self.epsilon = epsilon  # The Possibility for Exploration
        self.max_size = max_size  # The max_size for Q-table
        self.data = data_X  # The dataset for the Bayesian Network Learning
        self.variables = list(self.data.columns.values) # The variables in the Baysian Network
        self.done_bonus = 0.3 # The bonus of finishing forming up the Bayesian Structural Learning

        self.tabu_list = deque(maxlen=1000)
        # The Q-Table: Memorize the experience of Q(S,A)ï¼š S is the networkx object, the A is generated by choose_action
        self.Q_table = {}

        self.current_model = BayesianNetwork()
        self.current_model.add_nodes_from(self.variables)

    def remember(self, state, action, reward, state_, done):
        # Calculate maxQ(s',a'),exploration = False for maximum
        action_ = self._choose_action(state_, exploration = False)
        #Update the Q-value
        self.Q_table[(state, action)] = self.Q_table.get((state, action),0) + self.alpha * (reward + self.gamma *
                                                                     self.Q_table.get((state_, action_),0) -
                                                                     self.Q_table.get((state, action),0))


    def _legal_operations(
            self,
            model,
            score,
            structure_score,
    ):
        """Generates a list of legal (= not in tabu_list) graph modifications
        for a given model, together with their score changes. Possible graph modifications:
        (1) add, (2) remove, or (3) flip a single edge. For details on scoring
        see Koller & Friedman, Probabilistic Graphical Models, Section 18.4.3.3 (page 818).
        If a number `max_indegree` is provided, only modifications that keep the number
        of parents for each node below `max_indegree` are considered. A list of
        edges can optionally be passed as `black_list` or `white_list` to exclude those
        edges or to limit the search.
        """
        max_indegree = float("inf")
        tabu_list = set(self.tabu_list)

        # Step 1: Get all legal operations for adding edges.
        potential_new_edges = (
                set(permutations(self.variables, 2))
                - set(model.edges())
                - set([(Y, X) for (X, Y) in model.edges()])
        )

        for X, Y in potential_new_edges:
            # Check if adding (X, Y) will create a cycle.
            if not nx.has_path(model, Y, X):
                operation = ("+", (X, Y))
                if (
                        (operation not in tabu_list)
                ):
                    old_parents = model.get_parents(Y)
                    new_parents = old_parents + [X]
                    if len(new_parents) <= max_indegree:
                        score_delta = score(Y, new_parents) - score(Y, old_parents)
                        score_delta += structure_score("+")
                        yield (operation, score_delta)

        # Step 2: Get all legal operations for removing edges
        for X, Y in model.edges():
            operation = ("-", (X, Y))
            if (operation not in tabu_list):
                old_parents = model.get_parents(Y)
                new_parents = [var for var in old_parents if var != X]
                score_delta = score(Y, new_parents) - score(Y, old_parents)
                score_delta += structure_score("-")
                yield (operation, score_delta)

        # Step 3: Get all legal operations for flipping edges
        for X, Y in model.edges():
            # Check if flipping creates any cycles
            if not any(
                    map(lambda path: len(path) > 2, nx.all_simple_paths(model, X, Y))
            ):
                operation = ("flip", (X, Y))
                if (
                        ((operation not in tabu_list) and ("flip", (Y, X)) not in tabu_list)
                ):
                    old_X_parents = model.get_parents(X)
                    old_Y_parents = model.get_parents(Y)
                    new_X_parents = old_X_parents + [Y]
                    new_Y_parents = [var for var in old_Y_parents if var != X]
                    if len(new_X_parents) <= max_indegree:
                        score_delta = (
                                score(X, new_X_parents)
                                + score(Y, new_Y_parents)
                                - score(X, old_X_parents)
                                - score(Y, old_Y_parents)
                        )
                        score_delta += structure_score("flip")
                        yield (operation, score_delta)

    def _choose_action(self, graph, exploration = True, ):
        # The observation is ordered_node, containing the "action taken", and statevisited
        # Greedy: if the graph in Q-table, find the highest value
        # No Exploration is pure greedy

        # Step 1.1: Check Score Function
        score = BicScore(data=self.data)
        #use cache
        score_fn = ScoreCache(score, self.data).local_score
        #without cache
        # score_fn = score.local_score


        # best_operation, best_score_delta = max(
        #     self._legal_operations(
        #         graph,
        #         score_fn,
        #         score.structure_prior_ratio,
        #     ),
        #     key=lambda t: t[1],
        #     default=(None, None),
        # )

        best_operation = None
        best_score_delta = None
        for (s,a),q_value in self.Q_table.items():
            if s == graph:
                if q_value > best_score_delta:
                    best_score_delta = q_value
                    best_operation = a


        # Epsilon: a random action in the available list
        if (np.random.random() < 0.3 and exploration) or (best_operation == None):
            action_list = []
            for i in self._legal_operations(
                graph,
                score_fn,
                score.structure_prior_ratio,
            ):
                action_list.append(i)
            if (len(action_list) > 0):
                random_index = random.randint(0,len(action_list)-1)
                best_operation, best_score_delta = action_list[random_index]
            else:
                best_operation, best_score_delta = (None,None)


        return best_operation, best_score_delta

    def learn(self):
        return

    def reset(self):
        self.current_model = BayesianNetwork()
        self.current_model.add_nodes_from(self.variables)
        self.tabu_list = deque(maxlen=1000)

        return self.current_model

    def estimate_once(self, start_dag):
        best_operation, best_score_delta = self._choose_action(graph=start_dag)
        # Take the action
        if best_operation is None:
            best_score_delta = 0
        elif best_operation[0] == "+":
            self.current_model.add_edge(*best_operation[1])
            self.tabu_list.append(("-", best_operation[1]))
        elif best_operation[0] == "-":
            self.current_model.remove_edge(*best_operation[1])
            self.tabu_list.append(("+", best_operation[1]))
        elif best_operation[0] == "flip":
            X, Y = best_operation[1]
            self.current_model.remove_edge(X, Y)
            self.current_model.add_edge(Y, X)
            self.tabu_list.append(best_operation)


        return self.current_model, best_operation